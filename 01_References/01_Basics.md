# Base Understanding of Kubernetes Storage

## ğŸ§± Setup Overview

Your setup structure:

```
Host Machine (Physical)
â”‚
â””â”€â”€ Virtual Machines (VMs)
     â”œâ”€â”€ Control-plane node (master)
     â”œâ”€â”€ Worker node 1
     â”œâ”€â”€ Worker node 2
     â””â”€â”€ Worker node 3
```

Each worker node runs its own Linux OS with **kubelet**, a **container runtime** (Docker or containerd), and pods.

When a **Pod** writes to `/mnt/data` (via a `hostPath` volume), it writes inside that nodeâ€™s local filesystem â€” **not** your physical host.

### ğŸ’¾ Where is `/mnt/data` stored?

Data path:

```
Pod â†’ /mnt/data (container)
â”‚
â””â”€â”€ hostPath /mnt/data (worker node)
     â”‚
     â””â”€â”€ virtual disk (e.g., .qcow2, .vmdk)
          â”‚
          â””â”€â”€ stored inside host machine as a VM image file
```

So the data ends up on your host **inside the VMâ€™s disk image**, not in a visible folder.

### ğŸ§° Accessing `/mnt/data`

You cannot access it directly from your host; instead, use one of these:

```bash
kubectl debug node/<node-name> --image=busybox
# OR
ssh <node-vm>
# OR
virsh console <node-name>
```

---

## âš™ï¸ Problem Scenario

You have **1 control-plane + 5 worker nodes**. Some pods store data in **PersistentVolumes (PVs)** using `hostPath`. When scaling down (removing worker nodes), you fear losing data.

### ğŸ§¨ Issue â€” hostPath is Node-Bound

A `hostPath` volume keeps data **only on that specific nodeâ€™s disk**:

* If node is removed from cluster â†’ data remains but pods canâ€™t reach it.
* If VM or node is deleted â†’ data is **lost permanently**.

So `hostPath` is suitable **only for testing**, not production.

---

## ğŸ§­ Production Storage Solutions

Use storage independent of any node.

| Storage Type                  | Example                            | Description                   |
| ----------------------------- | ---------------------------------- | ----------------------------- |
| **NFS (Network File System)** | Shared drive                       | Accessible from all nodes     |
| **Cloud Block Storage**       | AWS EBS, GCP Disk, Azure Disk      | Auto-attaches to new node     |
| **Distributed Storage**       | Ceph, GlusterFS, Longhorn, OpenEBS | Replicated & fault-tolerant   |
| **Dynamic StorageClass**      | NFS/EBS provisioners               | Creates volumes automatically |

---

## ğŸ§  Safe Node Removal Options

### **Option 1: Move Data to Shared Storage**

1. Create a PV using NFS or another shared backend.
2. Bind a PVC to it.
3. Copy old data from hostPath PV to new PV (via `rsync` or a temporary pod).
4. Update deployments to use the new PVC.
5. Safely delete the old nodes.

### **Option 2: Backup Before Deletion**

If still using `hostPath`:

```bash
kubectl drain <node-name> --ignore-daemonsets
ssh <node>
tar czvf /tmp/backup.tar.gz /mnt/data
```

Then copy the backup elsewhere before removing the node.

---

## ğŸ§° Summary

| Case                  | Data Location           | If Node Deleted | Recommended Fix             |
| --------------------- | ----------------------- | --------------- | --------------------------- |
| hostPath PV           | Inside node (/mnt/data) | âŒ Lost          | Move to NFS / Cloud storage |
| Dynamic PV            | Cloud disk              | âœ… Persists      | Auto reattaches             |
| NFS / Ceph / Longhorn | Shared storage          | âœ… Available     | Best for on-prem            |

### âœ… Conclusion

Before removing worker nodes, **migrate data** to shared or cloud-based storage. Avoid using `hostPath` for any environment where scaling or node replacement may occur.

---

## âš¡ Local PVs and Node Affinity

When you use **local storage PVs**, Kubernetes must ensure pods run on the node that owns that PV.

### ğŸ’¡ Why

Local PVs reference a physical path on a node. If the pod runs elsewhere, the data path doesnâ€™t exist â†’ pod fails.

### âœ… Solution â€” Use Node Affinity

Specify node affinity in the PV definition.

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: localpv-worker1
spec:
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  storageClassName: local-storage
  persistentVolumeReclaimPolicy: Retain
  local:
    path: /mnt/data
  nodeAffinity:
    required:
      nodeSelectorTerms:
        - matchExpressions:
            - key: kubernetes.io/hostname
              operator: In
              values:
                - worker1
```

This ensures Kubernetes schedules any pod using this PV **only** on `worker1`, where the data physically exists.
